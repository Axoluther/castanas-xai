{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545285b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:24:49.466446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "import numpy as np\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7ef103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_data(dir, size):\n",
    "    imgs = []\n",
    "    names = []\n",
    "    for nombre in os.listdir(dir):\n",
    "        ruta = os.path.join(dir, nombre)\n",
    "        img = load_img(ruta, target_size=size)\n",
    "        img_array = img_to_array(img).astype(np.uint8)\n",
    "        imgs.append(img_array)\n",
    "        names.append(os.path.splitext(nombre)[0])  # sin extensión\n",
    "    return imgs, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f513b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img_data(imgs, names, dst, it , original:bool):\n",
    "    typ = \"orig\" if original else \"aug\"\n",
    "    for i, img_array in enumerate(imgs):\n",
    "        img = PIL.Image.fromarray(img_array)\n",
    "        name = f\"{names[i]}_{typ}.jpg\" if it ==0 else f\"{names[i]}_{it}_{typ}.jpg\"\n",
    "        img.save(os.path.join( dst, name ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e06c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(in_dir, out_dir, size, it, save_orig:bool = False):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Carga de imagenes de la carpeta\n",
    "    imgs, names = load_img_data(in_dir, size)\n",
    "\n",
    "    # Convertimos a batch tensor \n",
    "    batch = tf.convert_to_tensor(imgs, dtype=tf.uint8)\n",
    "\n",
    "    # Parámetros para augmentación\n",
    "    data_augmentation = Sequential([\n",
    "        layers.CenterCrop(512, 512), # Recorta el centro, priorizando el elemento central\n",
    "        layers.Resizing(256, 256),   # Se reduce recién la imagen\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomZoom(0.2),\n",
    "        layers.RandomTranslation(0.1, 0.1),\n",
    "        layers.RandomContrast(0.2),\n",
    "        layers.RandomBrightness(0.2),\n",
    "    ])\n",
    "\n",
    "    # Convertimos a float32 para aplicar augmentación (requerido por Keras)\n",
    "    batch_float = tf.cast(batch, tf.float32)\n",
    "\n",
    "    # Paso 4: Aplicar augmentación vectorizada\n",
    "    batch_aug = data_augmentation(batch_float, training=True)\n",
    "    batch_aug = tf.clip_by_value(batch_aug, 0.0, 255.0)\n",
    "    batch_aug = tf.cast(batch_aug, tf.uint8)\n",
    "    batch_aug = batch_aug.numpy()\n",
    "\n",
    "    # Guardamos los datos originales en el dataset\n",
    "    if save_orig:\n",
    "        save_img_data(imgs, names, out_dir, it, original=True)\n",
    "\n",
    "    # Guardamos augmentacion\n",
    "    save_img_data(batch_aug, names, out_dir, it, original=False)\n",
    "\n",
    "    msg = _ \n",
    "    if save_orig: \n",
    "        msg = f\"Guardadas {len(batch)} imágenes originales y {len(batch_aug)} augmentadas en '{out_dir}'\"\n",
    "    else: \n",
    "        msg = f\"Guardadas {len(batch_aug)} imgágenes augmentated en {out_dir}\"\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7c3d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:26:10.556107: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 581959680 exceeds 10% of free system memory.\n",
      "2025-05-19 16:26:39.497571: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1163919360 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas 370 imágenes originales y 370 augmentadas en 'castanas_aug_ds/comible'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:27:26.458488: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 581959680 exceeds 10% of free system memory.\n",
      "2025-05-19 16:27:56.406508: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1163919360 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas 370 imgágenes augmentated en castanas_aug_ds/comible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 16:28:41.591624: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 581959680 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardadas 370 imgágenes augmentated en castanas_aug_ds/comible\n",
      "Guardadas 177 imágenes originales y 177 augmentadas en 'castanas_aug_ds/no_comible'\n",
      "Guardadas 177 imgágenes augmentated en castanas_aug_ds/no_comible\n",
      "Guardadas 177 imgágenes augmentated en castanas_aug_ds/no_comible\n"
     ]
    }
   ],
   "source": [
    "dataset = \"castanas_ds\"\n",
    "dataset_aug = \"castanas_aug_ds\"\n",
    "class1 = \"comible\"\n",
    "class2 = \"no_comible\"\n",
    "size = (256,256)\n",
    "\n",
    "# Aumento de datos clase 1\n",
    "data_augmentation(f\"{dataset}/{class1}\", f\"{dataset_aug}/{class1}\", it=0, size=size, save_orig=True )\n",
    "data_augmentation(f\"{dataset}/{class1}\", f\"{dataset_aug}/{class1}\", it=1, size=size )\n",
    "data_augmentation(f\"{dataset}/{class1}\", f\"{dataset_aug}/{class1}\", it=2, size=size )\n",
    "\n",
    "# Auento de datos clase 2\n",
    "data_augmentation(f\"{dataset}/{class2}\", f\"{dataset_aug}/{class2}\", it=0, size=size, save_orig=True )\n",
    "data_augmentation(f\"{dataset}/{class2}\", f\"{dataset_aug}/{class2}\", it=1, size=size )\n",
    "data_augmentation(f\"{dataset}/{class2}\", f\"{dataset_aug}/{class2}\", it=2, size=size )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
